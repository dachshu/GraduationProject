# 한 일
- NMT를 이용한 학습, 댓글 생성 작업을 자동화 스크립트에서 제외시킴
NMT의 생성 결과가 좋지 않았음
- Bert를 이용해서 추천 사용자의 성향을 파악하는 스크립트 작성
대부분 계정의 연관성이 30% 이하일 정도로 학습 결과가 좋지 않고,
스크립트가 log에 기록을 제대로 남기지 않는 버그가 있음
- Bert의 성능을 높이기 위해 새로나온 wwm(Whole Word Masking)-big 모델을 사용해서 다시 학습
학습 데이터는 기존과 같이 임의로 선정한 보수,진보 성향 각각 10개의 트위터 계정에서 뽑아낸 트윗들.
약 50시간 동안 학습. (batch_size = 25, epoch = 50)
그러나 Training 결과는 더 나빠짐. eval data 기준으로, 기존의 70% 보다 더 낮은 49%.

# 할 일
- Bert의 모델 뿐 아니라 학습 데이터의 질을 높일 필요가 있음
트위터 계정의 트윗들 대신 네이버, 다음 정치 기사 댓글들을 학습 데이터로 사용.
그러면 학습 데이터 양을 크게 늘릴 수 있음. 대신 질 나쁜 댓글들을 거르기 위해서 한 기사당 소수의 상위 댓글(약 10개?)만을 뽑아내고, 장기간의 기사들을 크롤링 해서 데이터 양을 채움.